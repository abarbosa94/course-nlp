{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Due on: 5/30.  Please upload your completed assignment to Canvas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. For the following words: logistic, logistics, shoe, shoes\n",
    "\n",
    "   a. Porter stem with nltk\n",
    "   \n",
    "   b. lemmatize with nltk\n",
    "   \n",
    "   c. lemmatize with Spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = ['logistic', 'logistics', 'shoe', 'shoes']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer, WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['logist', 'logist', 'shoe', 'shoe']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[stemmer.stem(word) for word in words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemma = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['logistic', 'logistics', 'shoe', 'shoe']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[lemma.lemmatize(word) for word in words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(\" \".join(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['logistic', 'logistic', 'shoe', 'shoe']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[token.lemma_ for token in doc]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2\\. n-grams are an important NLP concept.  An n-gram is a contiguous sequence of n items (where the items can be characters, syllables, or words).  Here, we  A 1-gram is a unigram, a 2-gram is a bigram, and a 3-gram is a trigram.\n",
    "\n",
    "Here, we are referring to sequences of words. The sentence \"It was a bright cold day in April.\" contains the following trigrams:\n",
    "\n",
    "- It was a\n",
    "- was a bright\n",
    "- a bright cold\n",
    "- bright cold day\n",
    "- cold day in\n",
    "- day in April\n",
    "\n",
    "Write a function that returns a dictionary with the n-grams of a text (for `min_n <= n <= max_n`) and a count of how often they appear:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "expression = \"The book is on the table\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "def get_ngrams(text, min_n, max_n):  \n",
    "    #Exercise: FILL IN METHOD\n",
    "    ngram_dict = []\n",
    "    words = text.split()\n",
    "    if min_n<1:\n",
    "        min_n = 1\n",
    "    for j in range(len(words)):\n",
    "        for i in range(min_n,max_n+1):\n",
    "            if i+j>len(words):\n",
    "                break\n",
    "            ngram_dict.append(\" \".join(words[j:i+j]))\n",
    "    \n",
    "    return dict(Counter(ngram_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'The book is on': 1, 'book is on the': 1, 'is on the table': 1}"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_ngrams(expression,4,4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3\\. Write a method that given a list of strings (you can think of each string as a document), returns a dictionary for each string, where the keys are the vocabulary words, and the values are the frequencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [],
   "source": [
    "strings = [\"This is an awesome string an aa\", \"This another awesome string an\", \"This is aa third awesome string\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "def get_vocab_frequency(list_of_strings):\n",
    "    list_of_dicts = []\n",
    "    for doc in list_of_strings:\n",
    "        list_of_dicts.append(dict(Counter(doc.split())))\n",
    "\n",
    "    return list_of_dicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'This': 1, 'is': 1, 'an': 2, 'awesome': 1, 'string': 1, 'aa': 1},\n",
       " {'This': 1, 'another': 1, 'awesome': 1, 'string': 1, 'an': 1},\n",
       " {'This': 1, 'is': 1, 'aa': 1, 'third': 1, 'awesome': 1, 'string': 1}]"
      ]
     },
     "execution_count": 275,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_vocab_frequency(strings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4\\. Write a method that when given a list of strings (you can think of each string as a document), calculates the TF-IDF, and returns a term-document matrix with the results. It will be useful to use your `get_vocab_frequency` method from problem 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "def get_tfidf(list_of_strings):\n",
    "    docs = get_vocab_frequency(list_of_strings)\n",
    "    document_frequency = Counter()\n",
    "    for doc in docs:\n",
    "        document_frequency.update(doc.keys())\n",
    "    \n",
    "    sti = {word:idx for idx,word in enumerate(document_frequency,0)}\n",
    "    tfidf = np.zeros( (len(document_frequency), len(docs)))\n",
    "    for i in range(len(docs)):\n",
    "        doc = docs[i]\n",
    "        for word in doc:\n",
    "            tfidf[sti[word]][i] = doc[word] * (np.log(len(docs) / document_frequency[word]) + 1)    \n",
    "    return tfidf, sti"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf, sti = get_tfidf(strings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_sklearn = TfidfVectorizer(norm=None, smooth_idf=False, vocabulary=sti, lowercase=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 378,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.allclose(tfidf_sklearn.fit_transform(strings).todense().T, tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5\\. Who said the following (*Hint: Be sure to read the class notebooks and relevant links*):\n",
    "\n",
    "A. \"It's true there's been a lot of work on trying to apply statistical models to various linguistic problems. I think there have been some successes, but a lot of failures. There is a notion of success ... which I think is novel in the history of science. It interprets success as approximating unanalyzed data.\" -> **Chonksy**\n",
    "\n",
    "B. \"I agree that it can be difficult to make sense of a model containing billions of parameters. Certainly a human can't understand such a model by inspecting the values of each parameter individually. But one can gain insight by examing the properties of the model—where it succeeds and fails, how well it learns as a function of data, etc.\" -> **Norvig**\n",
    "\n",
    "C. The big-data big-compute paradigm of modern Deep Learning has in fact “perverted the field” (of computational linguistics) and “sent it off-track” -> **Cristopher Manning**\n",
    "\n",
    "D. Language is crucial to general intelligence, because language is the conduit by which individual intelligence is shared and transformed into societal intelligence. -> **Cristopher Manning**\n",
    "\n",
    "E. Structure is a “necessary evil”, and warned that imposing structure requires us to make certain assumptions, which are invariably wrong for at least some portion of the data, and may become obsolete within the near future. -> **LeCun**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
